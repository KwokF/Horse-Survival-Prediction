---
title: "ST310 Group Project Horse Dataset"
output:
  pdf_document: default
  pdf: default
  html_document:
    df_print: paged
always_allow_html: TRUE
---
Candidate Numbers: XXXXX, XXXXX, XXXXX, XXXXX

# Introduction

(Somewhat of an introduction and include the research questions that we are aiming to answer here, motivation of study etc.)

(add bolds to emphasize main points etc.)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Dataset
(Describe the dataset)

```{r message=FALSE}
library(tidyverse)
myData <- read_csv("horse.csv")
head(myData)
```

# Data Cleaning
The data cleaning process for our ST310 Group Project was an essential step in preparing our dataset for analysis. Our primary objective was to ensure data quality and readiness for answering our research questions, which focuses on understanding the factors influencing the outcomes for our subjects.

In our data cleaning process, one of the significant steps involved was the extraction and categorization of lesion information from a single, compact representation in the `type of lesion` column. Each entry in this column, represented by a code such as `4124`, encapsulates multiple pieces of information about the lesion: the site, type, subtype, and specific code related to the lesion characteristics. To make this data more analysable, we decomposed this compact code (based on the dataset documentation) into 4 separate columns, `lesion_site`, `lesion_type`, `lesion_subtype`, `lesion_code`, thereby enhancing the granularity of our dataset for more nuanced analysis. This step was crucial for enabling detailed statistical analysis and insights into the relationship between lesion characteristics and outcomes.

We also prepared the data by converting the relevant columns to factors or numeric. We dropped unnecessary columns, including identifiers and dropped the columns `lesion_2` and `lesion_3` as they had less than 10 rows which were not NA. 

```{r}
preprocessor <- function(dataframe) {
  dataframe %>%
    #janitor::clean_names() %>%
    pivot_longer(starts_with("lesion"),  #=
                 names_to = "lesion_id") %>%
    filter(!is.na(value)) %>%

    mutate(  # Need to process lesion columns since each number represents information and should not be treated as numerical!
      lesion_site = case_when(
        str_sub(as.character(value), 1, 2) == "11" & str_length(value) == 5 ~ "all intestinal",
        str_sub(as.character(value), 1, 1) == "1" ~ "gastric",
        str_sub(as.character(value), 1, 1) == "2" ~ "sm intestine",
        str_sub(as.character(value), 1, 1) == "3" ~ "lg colon",
        str_sub(as.character(value), 1, 1) == "4" ~ "lg colon and cecum",
        str_sub(as.character(value), 1, 1) == "5" ~ "cecum",
        str_sub(as.character(value), 1, 1) == "6" ~ "transverse colon",
        str_sub(as.character(value), 1, 1) == "7" ~ "return colon",
        str_sub(as.character(value), 1, 1) == "8" ~ "uterus",
        str_sub(as.character(value), 1, 1) == "9" ~ "bladder",
        str_sub(as.character(value), 1, 1) == "0" ~ "none",
        TRUE ~ "ERROR"
      ),
      startpoint = if_else(str_length(value) == 5 & str_sub(as.character(value), 1, 2) == "11", 3, 2),
      value2 = str_sub(value, startpoint, -1),
      lesion_type = case_when(
        str_sub(as.character(value2), 1, 1) == "1" ~ "simple",
        str_sub(as.character(value2), 1, 1) == "2" ~ "strangulation",
        str_sub(as.character(value2), 1, 1) == "3" ~ "inflammation",
        str_sub(as.character(value2), 1, 1) == "4" ~ "other",
        lesion_site == "none" ~ "none",
        TRUE ~ "ERROR"
      ),
      value3 = str_sub(value2, 2, -1),
      lesion_subtype = case_when(
        str_sub(as.character(value3), 1, 1) == "1" ~ "mechanical",
        str_sub(as.character(value3), 1, 1) == "2" ~ "paralytic",
        str_sub(as.character(value3), 1, 1) == "0" ~ NA_character_,
        lesion_site == "none" ~ "none",
        TRUE ~ "ERROR"
      ),
      value4 = str_sub(value3, 2, -1),
      lesion_code = case_when(
        str_sub(as.character(value4), 1, 1) == "1" ~ "obturation",
        str_sub(as.character(value4), 1, 1) == "2" ~ "intrinsic",
        str_sub(as.character(value4), 1, 1) == "3" ~ "extrinsic",
        str_sub(as.character(value4), 1, 1) == "4" ~ "adynamic",
        str_sub(as.character(value4), 1, 1) == "5" ~ "volvulus/torsion",
        str_sub(as.character(value4), 1, 1) == "6" ~ "intussuption",
        str_sub(as.character(value4), 1, 1) == "7" ~ "thromboembolic",
        str_sub(as.character(value4), 1, 1) == "8" ~ "hernia" ,
        str_sub(as.character(value4), 1, 1) == "9" ~ "lipoma/slenic incarceration",
        str_sub(as.character(value4), 1, 1) == "0" ~ NA_character_,
        str_sub(as.character(value4), 1, 2) == "10" ~ "displacement",
        lesion_site == "none" ~ "none",
        TRUE ~ "ERROR",
        
      )
    )  %>%
    
    mutate(
      hospital_number = as_factor(hospital_number), #The hospital number is a categorical variable because it acts as in ID for different hospitals

      
      #the remaining categorical variables are converted to factors
      pain = as.factor(pain),
      nasogastric_reflux = as.factor(nasogastric_reflux),
      surgery = as.factor(surgery),
      age = as.factor(age),
      temp_of_extremities = as.factor(temp_of_extremities),
      peripheral_pulse = as.factor(peripheral_pulse),
      mucous_membrane = as.factor(mucous_membrane),
      capillary_refill_time = as.factor(capillary_refill_time),
      peristalsis = as.factor(peristalsis),
      abdominal_distention = as.factor(abdominal_distention),
      nasogastric_tube = as.factor(nasogastric_tube),
      rectal_exam_feces = as.factor(rectal_exam_feces),
      abdomen = as.factor(abdomen),
      abdomo_appearance = as.factor(abdomo_appearance),
      outcome = as.factor(outcome),
      surgical_lesion = as.factor(surgical_lesion)) %>% 

  mutate(id = row_number()) %>% 
  select(-cp_data, -hospital_number, -value, -startpoint, -value2, -value3, -value4)
}

library(readr)
library(dplyr)


raw_df <- myData %>% 
  distinct(across(everything()), .keep_all = TRUE) %>%
  preprocessor()  %>%
  dplyr::slice(seq(1, n(), by = 3)) %>%
  select(-lesion_id, -id) %>%
  mutate(
    lesion_site = factor(lesion_site),
    lesion_type = factor(lesion_type),
    lesion_subtype = as.factor(lesion_subtype),
    lesion_code = as.factor(lesion_code) 
  
  ) 
head(raw_df)
```

## Merging
In the merging process of our data cleaning, we focused on enhancing the interpretability of our models by consolidating categorical variables. This step involved merging categories within variables based on their inherent ordered structure or based on external information about their relationships. For instance, temperature extremities like 'cool' and 'cold' were merged to reflect a more interpretable category. Similar merging was applied to mucous membrane colors, peristalsis levels, abdominal distention, and lesion sites to simplify the dataset while retaining meaningful distinctions. 

Additionally, we made the decision to drop variables `lesion type`, `lesion_subtype`, `lesion_code`. This decision was driven by the recognition of inconsistencies and inaccuracies in the data, particularly stemming from the data collection phase. Upon closer examination, it became evident that these columns exhibited discrepancies that could potentially compromise the integrity and reliability of our analysis.

(do we want to go into specifics????)
```{r}
pre_merge_df <- raw_df

library(car)
# Merging the categorical variables to increase the interpretability of the models
# Merged based on intuition, especially if there is some sort of ordered structure in the levels
# Also if there was information provided on the dataset of the relationship of the levels
raw_df$temp_of_extremities <- with(raw_df, Recode(temp_of_extremities, "c('cool', 'cold') = 'cool/cold'"))

raw_df$mucous_membrane <- with(raw_df, Recode(mucous_membrane, "c('normal_pink', 'bright_pink') = 'normal/bright pink'"))
raw_df$mucous_membrane <- with(raw_df, Recode(mucous_membrane, "c('pale_cyanotic', 'dark_cyanotic') = 'pale/dark cyanotic'"))

raw_df$peristalsis <- with(raw_df, Recode(peristalsis, "c('hypermotile', 'normal') = 'hypermotile/normal'"))
raw_df$peristalsis <- with(raw_df, Recode(peristalsis, "c('hypomotile', 'absent') = 'hypomotile/absent'"))

raw_df$abdominal_distention <- with(raw_df, Recode(abdominal_distention, "c('none', 'slight') = 'none/slight'"))
raw_df$abdominal_distention <- with(raw_df, Recode(abdominal_distention, "c('moderate', 'severe') = 'moderate/severe'"))

raw_df$abdomen <- with(raw_df, Recode(abdomen, "c('distend_large', 'distend_small') = 'distend large/small'"))
raw_df$abdomen <- with(raw_df, Recode(abdomen, "c('firm', 'other') = 'other'"))

raw_df$lesion_site <- with(raw_df, Recode(lesion_site, "c('lg colon', 'transverse colon', 'cecum', 'lg colon and cecum', 'return colon') = 'colon/cecum'"))
raw_df$lesion_site <- with(raw_df, Recode(lesion_site, "c('bladder', 'all intestinal', 'gastric', 'uterus') = 'other'"))

raw_df$capillary_refill_time <- with(raw_df, Recode(capillary_refill_time, "c('3', 'more_3_sec') = '3_or_more'"))

raw_df$capillary_refill_time <- with(raw_df, Recode(capillary_refill_time, "c('less_3_sec') = '3_or_less'"))

table(raw_df$lesion_type)

# Dropping lesion_type lesion_subtype and lesion_code due to faulty data
raw_df <- raw_df[,-c(25, 26, 27)]
```


## Final Dataset
Below is the final cleaned dataset.
```{r}
horse_data <- raw_df
```

# EDA
Below is some exploratory data analysis to get a better understanding of our dataset. 

## Outcome Variable
This a bar plot to visualise the outcome variable. There are 3 possibilities for the outcome variable, `died`, `euthanized`, and `lived`. We notice there is some class inblance in the data, ()......
```{r}
library(ggplot2)

# Plotting the outcome variable
ggplot(data = horse_data, aes(x = outcome, fill = outcome)) +
  geom_bar() + # Bar Plot
  labs(title = "Bar Plot by Horse Outcome", x = "Outcome", y = "Count") # Adding the labels
```


## Correlation Matrix
Below is the correlation matrix between the continuous predictors. Overall, there aren't many strong relationships among the continuous variables. However, the most prominent one observed is a strong negative correlation between `nasogastric_reflux_ph` and `total_protein`, with a correlation coefficient of -0.72. Additionally, there are positive correlations between `nasogastric_reflux_ph` and `abdomo_protein`, as well as between `pulse` and `respiratory_rate`. Conversely, there is a negative correlation between `abdomo_protein` and `total_protein`.
```{r}
library(corrplot)

numeric_columns <- sapply(horse_data, is.numeric) # Finding the continuous predictors
df_numeric <- subset(horse_data, select = numeric_columns) 
correlation_matrix <- cor(df_numeric, use = "pairwise.complete.obs") # Creating the correlation matrix

# Using the corrpot function to plot the matrix, only keeping the upper triangular matrix to avoid duplicate/unecessary values 
corrplot(correlation_matrix, method = "color", type = "upper", order = "hclust", tl.col = "black",  
         tl.srt = 45, col = colorRampPalette(c("red", "white", "blue"))(100), addCoef.col = "black",  
         number.cex = 0.7, cl.pos = "n", diag = FALSE)

```

## Plots
Below are plots to visualise the relationship of each predictor with the outcome variable.

### Density Plots
Below are the density plots illustrating the influence of the continuous variables on the categorical outcome. An interesting observation was that horses that ultimately survived generally showed lower `pulse` rates and a lower `packed_cell_volume` compared to those that were euthanized or deceased. However, the predictive power of `respiratory_rate` appears to be less significant, as the distributions for the categories `died`, `euthanized`, and `lived` are relatively similar.
```{r warning=FALSE}
horse_data %>% 
  select(where(is.numeric), outcome) %>% # Selecting numeric rows
  pivot_longer(- c(outcome),
    names_to = "metric",
    values_to = "value"
  ) %>%
  ggplot(aes(value, color = outcome)) + # Plotting the density plots
  geom_density(alpha = 0.5) +
  facet_wrap(vars(metric), scales = "free", ncol = 3) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```

## Box Plots
Below are some box plots depicting the relationships between the continuous variables and `outcome`. In general, most of the box plots do not exhibit a strong, clear relationship between these predictors and the final outcome for the horse. However, the plot of `Outcome` versus `packed_cell_volume` does reveal some level of ordering among the outcomes. It appears that a lower `packed_cell_volume` corresponds to a higher chance of survival, which aligns with our observations from the density plots.

```{r warning = FALSE}
library(patchwork)
continuous_vars <- horse_data %>%
  select(where(is.numeric)) #

# pdf("output_plots.pdf", width = 8, height = 10)

plots_continuous_box <- map(names(continuous_vars), 
                            ~ ggplot(raw_df, aes(x = outcome, y = .data[[.x]], fill = outcome)) +
                              geom_boxplot() +
                              labs(title = paste("Outcome vs", .x), y = .x) +
                              theme_minimal() +
                              theme(axis.text.x = element_text(angle = 45, hjust = 1))
                            )

par(mfrow = c(2, 2))
wrap_plots(plots_continuous_box[1:4])
par(mfrow = c(2, 2))
wrap_plots(plots_continuous_box[5:7], ncol = 2)
```

## Bar Plots
Below are some visualizations illustrating the impact of each categorical predictor on the `outcome`. While there weren't many distinct patterns observed within the categorical predictors from these plots, it's noteworthy that a significant portion of the surviving horses exhibited a `capillary_refill_time` of less than 3 and 'none/slight' abdominal distention.
```{r}
categorical_vars <- select(horse_data, -outcome) %>% # Extracting the categorical variables
  select(where(~!is.numeric(.) & !is.logical(.)))

# Generate bar plots for categorical variables
plots_categorical <- map(names(categorical_vars), ~ggplot(raw_df, aes_string(x = .x, fill = 'outcome')) +
                           geom_bar(position = "dodge") +
                           labs(title = paste("Outcome vs", .x), x = .x) +
                           theme_minimal() + # Adjusting text sizes
                           theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 6), 
                                 axis.text.y = element_text(size = 6),
                                 axis.text = element_text(size = 6),
                                 plot.title = element_text(size = 7),
                                 axis.title = element_text(size = 7),
                                 legend.text = element_text(size = 6),  
                                 legend.title = element_text(size = 6),
                                 legend.key.size = unit(0.5, "lines")) 
)
wrap_plots(plots_categorical[1:9])
wrap_plots(plots_categorical[10:16])

```

# Missing Data
The dataset presents a significant challenge due to the abundance of missing data. Only 6 rows in the entire dataset had no missing values, which is particularly concerning given the relatively small size of the dataset, comprising approximately 299 rows. To address this issue, we adopted three distinct approaches, which are elaborated upon below. In the report, we will evaluate all models using these three methods and summarize the findings.

## Complete Case Analysis
For our first method, we conducted a complete case analysis. In this approach, columns with a significant number of missing values were excluded, using a threshold of 15%. This threshold was selected to minimize the loss of valuable data from the dataset. Ultimately, only 28% of the data was dropped, leaving 215 rows available for the complete case analysis.
```{r}
# Removing columns with more than 15% missing values

nas_per_column <- sapply(horse_data, function(x) sum(is.na(x)) / nrow(horse_data))
columns_to_remove <- names(nas_per_column[nas_per_column > 0.15])

horse_data_clean <- horse_data[, !(names(horse_data) %in% columns_to_remove)]

complete_case_df <- horse_data_clean[complete.cases(horse_data_clean), ]
nrow(complete_case_df) # Number of rows left (for complete case analysis)
```

## `missForest` Imputation
For our second method, we decided to utlize the `missForest` package. This package employs a random forest trained on the observed values to conduct nonparametric missing value imputation. By using this method, we ensure that all 299 rows in the dataset are retained, and no data is lost during model training

```{r}
library(tidyverse)
library(missForest) # Importing the missForest package 

set.seed(123)

horse_copy_imputed <- as.data.frame(raw_df)

non_numeric_cols <- sapply(horse_copy_imputed, function(x) !is.numeric(x)) # Imputing the missing data
horse_copy_imputed[non_numeric_cols] <- lapply(horse_copy_imputed[non_numeric_cols], as.factor) 

imputed_data <- missForest(horse_copy_imputed)

imputed_df <- imputed_data$ximp
sum(is.na(imputed_df)) # Check if there are any missing values remaining
```

## Using Mean and Mode
The last method we employed involved replacing missing values with the mean for continuous variables and the mode for categorical variables. This approach also ensured that there were no missing data points when fitting the models. However, a notable weakness of this method is that it may not adequately capture the variability and nuances present in the dataset. By imputing missing values with the mean or mode, we essentially ignore potential patterns and relationships within the data. Consequently, this method might lead to biased estimations and could impact the accuracy of our models.

```{r}
mean_mode_df <- raw_df

# Function to calculate mode
getMode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Looping over each column
for (col_name in names(mean_mode_df)) {
  # Check if the column is numeric
  if (is.numeric(mean_mode_df[[col_name]])) {
    # Impute missing numeric values with mean
    mean_mode_df[[col_name]][is.na(mean_mode_df[[col_name]])] <- mean(mean_mode_df[[col_name]], na.rm = TRUE)
  } else {
    # Impute missing non-numeric values with mode
    mode_value <- getMode(mean_mode_df[[col_name]][!is.na(mean_mode_df[[col_name]])])
    mean_mode_df[[col_name]][is.na(mean_mode_df[[col_name]])] <- mode_value
  }
}

sum(is.na(mean_mode_df)) # Check if there are any missing values remaining
```

## Final Dataset
In this report, for illustrative purposes, we will fit all the models using the `missForest` package as it was also utilized by various research papers with the same dataset. All the results with all the datasets will be reported at the end of the report.

```{r}
df <- imputed_df
```

```{r}
library(tidymodels)
## run this to split into training and testing 
set.seed(123) # for reproducibility
data_split <- initial_split(df, prop = 0.75)
final_df_training_data <- training(data_split)
test_data_df <- testing(data_split)
```

# Inbalanced Outcome Variable: Using `SMOTE`
(talk about why we used SMOTE, but not not the output in the report)
```{r}
## only run this if want smote 

library(themis)
library(recipes)
library(modeldata)

recipe_obj <- recipe(outcome ~ ., data = final_df_training_data) %>%
  step_impute_knn(all_predictors(), neighbors = 5) %>%
  step_smotenc(outcome, over_ratio = 1, neighbors = 5) %>%
  prep(data = final_df_training_data)


smote_final_df_training_data <- bake(recipe_obj, new_data = NULL)


# Before
table(final_df_training_data$outcome)

# After
table(smote_final_df_training_data$outcome)
```


# Model: Baseline
For our baseline model, we decided to a fit a multinomial logistic regression model which included all the parameters, no parameter selection was done. This model was chosen as our baseline due to its simplicity and direct applicability to multi-class classification problems. It serves as a straightforward method for comparison against more complex models, allowing us to evaluate the value of using mode advanced machine learning techniques. With a testing accuracy of XXX, the model performs relatively well, especially considering that random prediction of the `outcome` variable would only achieve a 33% accuracy rate.
```{r}
library(tidymodels)
library(parsnip)
library(nnet) # For multinominal

set.seed(123) # for reproducibility
train_data <- final_df_training_data
test_data <- test_data_df

multinom_model_spec <- multinom_reg() %>%
  set_engine("nnet") %>%
  set_mode("classification")

recipe <- recipe(outcome ~ ., data = train_data)

multinom_fit <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(multinom_model_spec) %>%
  fit(data = train_data)


predictions <- predict(multinom_fit, new_data = test_data, type = "prob")


# in order to get the predicted class based on the highest probability
test_data_predictions <- test_data %>%
  select(outcome) %>%
  bind_cols(predictions) %>%
  mutate(pred_class = max.col(select(., -outcome), ties.method = "first"))

levels_pred <- levels(test_data$outcome)
test_data_predictions$pred_class <- factor(test_data_predictions$pred_class, levels = 1:length(levels_pred), labels = levels_pred)

# for the corr matrix 
metrics <- yardstick::metrics(test_data_predictions, truth = outcome, estimate = pred_class)
conf_mat <- yardstick::conf_mat(test_data_predictions, truth = outcome, estimate = pred_class)


knitr::kable(metrics, caption = "Metrics for Multinominal Model")
knitr::kable(conf_mat$table, caption = "Confusion Matrix for Multinominal Model")
```

(coefficients of baseline)
```{r message = FALSE}
library(knitr)
library(kableExtra)
# Assuming 'model_coefficients' contains the coefficients from your model
# and it's already in a data frame format as previously shown
# Transpose the data frame to make outcome classes as columns

model_coefficients <- coef(multinom_fit$fit$fit$fit)

coefficients_transposed <- t(model_coefficients)

# Convert to data frame
coefficients_df <- as.data.frame(coefficients_transposed)

# Add row names as a new column for variable names (since transposing shifts variable names to row names)
coefficients_df$Variable <- rownames(coefficients_df)

# Reorder the columns if needed
desired_order <- c("lived", "euthanized")
coefficients_df <- coefficients_df[, desired_order]

# Now use kable() to print with kableExtra for styling
#kable(coefficients_df, format = "html", booktabs = TRUE) %>%
  #kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

knitr::kable(coefficients_df, caption = "Multinominal Logistic Regression Model")
```

# Model: Gradient Descent
This model is our own implementation of stochastic gradient descent on a multinominal logisitic regression model, which optimises the cross-entropy loss function iteratively. It first preprocesses that dataset by converting categorical variables into numeric variables. The model utlises functions for softmax calculation, cross-entropy loss computation, and gradient descent. Through SGD, the model iteratively updates weights to minimize the cross-entropy loss function, increasing its predictive accuracy. The training process will span over multiple epochs, with each epoch evaluating the model performance. The model's parameters have been carefully selected to optimize its performance. A `learning_rate` of 0.0001, along with 150 epochs and a `batch_size` of 8, were chosen to allow for convergence to a local minimum during training. These parameter choices aim to strike a balance between model efficiency and effectiveness, ensuring robust optimization while avoiding overfitting. The final testing accuracy of the model after running over 150 epochs was XXX.

(Bit of explanation and add comments)
```{r}
library(tidyverse)


set.seed(123) # for reproducibility

train_data_sgd <- final_df_training_data %>%
  mutate(across(where(is.character), as.factor))  # to make sure all character columns are converted to factors

# need to make it start from 0 to use with the softmax function
train_data_sgd$outcome <- as.numeric(factor(train_data_sgd$outcome)) - 1

test_data_sgd <- test_data
test_data_sgd$outcome <- as.numeric(factor(test_data_sgd$outcome)) - 1

train_data <- train_data_sgd
test_data <- test_data_sgd

# have to one-hot encode categorical variables
X_train <- model.matrix(~ . - 1 - outcome, data = train_data)  # The '- 1' removes intercept
y_train <- train_data$outcome

# based on the code from the lecture slides/previous problem set
softmax <- function(z) {
  z_max <- apply(z, 1, max)  # to find the max value in each row
  exp_z <- exp(z - z_max)  # subtract the max value from each row for numerical stability
  exp_z / rowSums(exp_z)
}

cross_entropy_loss <- function(X, y, W) {
  n <- nrow(X)
  logits <- X %*% W
  probs <- softmax(logits)
  epsilon <- 1e-9  # small constant to avoid log(0)
  -sum(log(probs[cbind(1:n, y + 1)] + epsilon)) / n
}


cross_entropy_gradient <- function(X, y, W) {
  n <- nrow(X)
  k <- ncol(W)
  logits <- X %*% W
  probs <- softmax(logits)
  y_mat <- matrix(0, n, k)
  y_mat[cbind(1:n, y + 1)] <- 1
  grad <- t(X) %*% (probs - y_mat) / n
  return(grad)
}

sgd_multinomial_logistic <- function(X, y, learning_rate = 0.0001, epochs = 150, batch_size = 8) {
  n <- nrow(X)
  p <- ncol(X)
  k <- length(unique(y))
  W <- matrix(rnorm(p * k), p, k)
  
  metrics <- data.frame(epoch = integer(0), loss = numeric(0), accuracy = numeric(0)) # to store metrics, including accuracy
  
  for (epoch in 1:epochs) {
    shuffled_indices <- sample(1:n)
    X_shuffled <- X[shuffled_indices, ]
    y_shuffled <- y[shuffled_indices]
    
    for (i in seq(1, n, by = batch_size)) {
      batch_indices = i:min(i + batch_size - 1, n)
      X_batch = X_shuffled[batch_indices, , drop = FALSE]
      y_batch = y_shuffled[batch_indices]
      
      gradient_batch <- cross_entropy_gradient(X_batch, y_batch, W)
      W <- W - learning_rate * gradient_batch
    }
    
    # calculating the loss and accuracy for each epoch
    loss <- cross_entropy_loss(X, y, W)
    predictions <- apply(X %*% W, 1, which.max) - 1  # minus 1 to align with zero-indexed 'y'
    correct_predictions <- sum(predictions == y)
    accuracy <- correct_predictions / n
    metrics <- rbind(metrics, data.frame(epoch = epoch, loss = loss, accuracy = accuracy))
    
    if (epoch %% 10 == 0) {
    cat("Epoch:", epoch, "Loss:", loss, "Accuracy:", accuracy, "\n")
  }
  }
  
  return(list(W = W, metrics = metrics))
}

results <- sgd_multinomial_logistic(X_train, y_train)

# plot of accuracy vs epochs
results$metrics %>%
  ggplot(aes(x = epoch, y = accuracy)) +
  geom_point() + geom_line() +
  ggtitle("Accuracy over Epochs") +
  ylab("Accuracy") + xlab("Epoch")
```

# Model: Non-Baseline (Interpretability)
Here are some non-baseline models primarily focused on interpretability.

## Single Decision Tree
A single decision tree was selected as a possible candidate model for its interpretability. This model offers clear insights into how different features influence the survival outcome of horses. This model satisfies the requirement for an interpretable model, as it is relatively easier to interpret compared to the baseline to explain the predictions based on the structure of the tree. One can simply view the tree and understand how the model is making its predictions. 

The model achieves a test accuracy of XXX, which is not significantly superior to the baseline. However, it still demonstrates a relatively high accuracy rate, considering its simplicity and focus on interpretability rather than predictive capability. Such models are valuable in real-world scenarios where understanding the inner workings of the model is crucial and preferable to relying on complex black-box algorithms.

(Description of the single decision trees)
```{r}
#### single decision tree
library(tidymodels)
library(rpart) # For decision_tree
library(knitr) # For kable

set.seed(123) # for reproducibility

# Assigning datasets
data_train <- final_df_training_data
data_test <- test_data_df

# Preprocess with a recipe
data_recipe <- recipe(outcome ~ ., data = data_train) %>%
  step_dummy(all_nominal_predictors()) #%>%
  #prep()

# Model specification with tunable parameters
data_tree <- decision_tree(
  tree_depth = tune(), 
  cost_complexity = tune(),
  min_n = tune()
) %>%
  set_engine("rpart") %>%
  set_mode("classification")

data_workflow_tree <- workflow() %>%
  add_recipe(data_recipe) %>%
  add_model(data_tree)

# Define the tuning grid
grid <- grid_regular(
  tree_depth(range = c(1, 5)),
  cost_complexity(c(-5, 0)), # Converting to log scale for tuning
  min_n(range = c(2, 20)),  
  levels = 5
)

# Cross-validation for tuning
data_cv <- vfold_cv(data_train, v = 10, strata = outcome)

# Perform the tuning
data_fit_tree <- tune_grid(
  data_workflow_tree,
  resamples = data_cv,
  grid = grid,
  metrics = metric_set(accuracy)
)

# Extract the best parameters
data_tree_best <- select_best(data_fit_tree, "accuracy")

# Finalize the model with the best parameters
data_tree_final <- finalize_workflow(data_workflow_tree, data_tree_best)

# Fit the final model on the training data
final_fit <- fit(data_tree_final, data = data_train)

# Evaluate the model on the test set
predictions <- predict(final_fit, new_data = data_test, type = "prob")
# Assuming the outcome variable in your test set is named "outcome"
data_test$predicted_class <- predict(final_fit, new_data = data_test, type = "class")$.pred_class

# Evaluate metrics
metrics <- yardstick::metrics(data_test, truth = outcome, estimate = predicted_class)
conf_mat <- yardstick::conf_mat(data_test, truth = outcome, estimate = predicted_class)

# Print the results
knitr::kable(metrics, caption = "Metrics for Single Tree Model")
knitr::kable(conf_mat$table, caption = "Confusion Matrix for Single Tree Model")

# Print the best hyperparameters
data_tree_best %>%
  knitr::kable()

```

```{r}
data_fit_tree %>% autoplot() # Tuning parameters of tree
```

Below is a plot of the diagram of the tree model. From this we can see that...(do after finalising the code)
```{r}
library(rpart.plot)

final_model <- pull_workflow_fit(final_fit)
tree_model <- final_model$fit

rpart.plot(tree_model, roundint = FALSE)
```

## Lasso Regression
Another model we choose to be a possible candidate that is relatively interpretable is Lasso Regression. Lasso applies a penalty on the absolute size of coefficients, effectively performing variable selection by shrinking the coefficients of the less important features to zero, effectively removing them from the model. This allows the model to identify the most relevant predictors, simplifying the model, making it more interpretable while maintaining good predictive performance on the test data. It is more interpretable relative to the baseline model as it is essentially the same model as the baseline, however some coefficients were shrunk to zero. This model has a testing accuracy of XXX, (also talk about confusion matrix)

```{r message = FALSE}
#### lasso 
#### lasso 


# using the code from the lecture slides/previous problem set


library(tidymodels)
library(glmnet)
library(broom)
library(knitr)


set.seed(123) # for reproducibility
train_data <- final_df_training_data
test_data <- test_data_df

recipe <- recipe(outcome ~ ., data = train_data) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors()) %>%  # for zero variance 
  step_normalize(all_numeric(), -all_outcomes()) 

# model specification for multinomial logistic regression
lasso_spec <- multinom_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# workflow
workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(lasso_spec)

cv <- vfold_cv(train_data, v = 5, strata = outcome)
penalty_range <- dials::penalty(range = c(-6, -1), trans = log10_trans())

# tuning 
tune_results <- tune_grid(
  workflow,
  resamples = cv,
  grid = dials::grid_latin_hypercube(penalty_range,
  size = 20),
  metrics = metric_set(yardstick::accuracy)
)

# to get the best result based on accuracy 
best_results <- select_best(tune_results, "accuracy")
best_lambda <- best_results$penalty

# makng the model again using the best lambda
final_lasso_spec <- multinom_reg(penalty = best_lambda, mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# refit on the whole training dataset
final_workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(final_lasso_spec) %>%
  fit(data = train_data)

predictions <- predict(final_workflow, new_data = test_data, type = "prob")


max_prob <- apply(predictions, 1, which.max)
test_data$.pred_class <- levels(test_data$outcome)[max_prob]

# model metrics
metrics <- sum(test_data$.pred_class == test_data$outcome)/nrow(test_data)

print(metrics)

# to see the coefficients
best_lambda_numeric <- as.numeric(best_lambda)

final_model <- final_workflow %>%
  pull_workflow_fit() %>%
  pluck("fit")

# extract coefficients as a list of matrices
coefficients_list <- coef(final_model, s = best_lambda_numeric)


combined_coefficients <- data.frame()

for (class_name in names(coefficients_list)) {
    # convert to df
    class_coefficients_df <- as.data.frame(as.matrix(coefficients_list[[class_name]]))
    # add column with class name 
    class_coefficients_df$class <- class_name
    class_coefficients_df$predictor <- rownames(as.matrix(coefficients_list[[class_name]]))
    combined_coefficients <- rbind(combined_coefficients, class_coefficients_df)
}

# Rename the columns appropriately
colnames(combined_coefficients)[1] <- "estimate"

# need to use pivot longer for df
combined_coefficients_df <- combined_coefficients %>%
  pivot_wider(names_from = class, values_from = estimate)


kable(combined_coefficients_df, caption = "Lasso")


library(yardstick)

# Assuming test_data$.pred_class contains the predicted classes
# and test_data$outcome contains the actual outcomes
test_data$.pred_class <- as.factor(test_data$.pred_class)
conf_matrix <- conf_mat(test_data, truth = outcome, estimate = .pred_class)

# Alternatively, use the kable function from knitr for a nicer format in markdown documents
knitr::kable(conf_matrix$table, caption = "Confusion Matrix for Lasso Model")

```


## Best Model: Non-Baseline (Interpretability)
(final winner and do some plots)

# Model: High-Dimensional
For the high-dimensional model, we are going to fit 3 penalized regression models. Lasso with interactions, ridge regression and ElasticNet Regression. All three of these methods add a penalty term to a model's objective function. It penalizes the complexity of the model or the size of the coefficients, which encourage simpler models and avoids overfitting. 

(Talk about why its suitable for the dataset)

## Lasso with Interactions
Our initial candidate for a high-dimensional model is a Lasso model, similar to the previous one. It adds a penalty term that is the sum of the absolute values of the coefficients multiplied by a tuning parameter. However, to increase the model's complexity, we explored several interaction terms based on our understanding of these variables in real life. Adding intereactions increases complexity to the basic lasso model, potentially uncovering intricate relationships between predictors and the response variable. Notably, interactions involving `age` and `surgery` significantly improved the model's predictive accuracy. The model achieves a test prediction accuracy of XXX, (confusion matrix if necessary).

(Describe the model interpretation if possible)
```{r message=FALSE}
#### lasso with interactions


# using the code from the lecture slides/previous problem set


library(tidymodels)
library(glmnet)
library(dplyr)

set.seed(123) # for reproducibility
train_data <- final_df_training_data
test_data <- test_data_df

# Start with specifying the recipe without the interaction in the formula
recipe <- recipe(outcome ~ ., data = train_data) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric(), -all_outcomes())

recipe <- recipe %>%
  step_mutate(age_surgery_interaction = age_young * surgery_yes) # adjust to change interactions

# model specification for multinomial logistic regression
lasso_spec <- multinom_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# workflow
workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(lasso_spec)

cv <- vfold_cv(train_data, v = 5, strata = outcome)
penalty_range <- dials::penalty(range = c(-6, -1), trans = log10_trans())

# tuning 
tune_results <- tune_grid(
  workflow,
  resamples = cv,
  grid = dials::grid_latin_hypercube(penalty_range,
  size = 20),
  metrics = metric_set(yardstick::accuracy)
)

# to get the best result based on accuracy 
best_results <- select_best(tune_results, "accuracy")
best_lambda <- best_results$penalty

# makng the model again using the best lambda
final_lasso_spec <- multinom_reg(penalty = best_lambda, mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# refit on the whole training dataset
final_workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(final_lasso_spec) %>%
  fit(data = train_data)

predictions <- predict(final_workflow, new_data = test_data, type = "prob")


max_prob <- apply(predictions, 1, which.max)
test_data$.pred_class <- levels(test_data$outcome)[max_prob]

# model metrics
metrics <- sum(test_data$.pred_class == test_data$outcome)/nrow(test_data)

print(metrics)



# to see the coefficients

best_lambda_numeric <- as.numeric(best_lambda)

final_model <- final_workflow %>%
  pull_workflow_fit() %>%
  pluck("fit")

# extract coefficients as a list of matrices
coefficients_list <- coef(final_model, s = best_lambda_numeric)


combined_coefficients <- data.frame()


for (class_name in names(coefficients_list)) {
    # convert to df
    class_coefficients_df <- as.data.frame(as.matrix(coefficients_list[[class_name]]))
    
    # add column with class name 
    class_coefficients_df$class <- class_name
    

    class_coefficients_df$predictor <- rownames(as.matrix(coefficients_list[[class_name]]))
    

    combined_coefficients <- rbind(combined_coefficients, class_coefficients_df)
}

# Rename the columns appropriately
colnames(combined_coefficients)[1] <- "estimate"

# need to use pivot longer for df
combined_coefficients_df <- combined_coefficients %>%
  pivot_wider(names_from = class, values_from = estimate)


kable(combined_coefficients_df, caption = "Lasso with Interactions")


library(yardstick)

# Assuming test_data$.pred_class contains the predicted classes
# and test_data$outcome contains the actual outcomes
test_data$.pred_class <- as.factor(test_data$.pred_class)
conf_matrix <- conf_mat(test_data, truth = outcome, estimate = .pred_class)

# Alternatively, use the kable function from knitr for a nicer format in markdown documents
knitr::kable(conf_matrix$table, caption = "Confusion Matrix for Lasso Model with Interactions")
```

## Ridge Regression
We applied Ridge Regression as our second candidate for our high dimensional (penalized regression) model. This method adds a penalty term that is the sum of the squares of the coefficients multiplied by a tuning parameter. Adding this penalty term usually encourages smaller coefficients, although it typically does not force them to be zero. Ridge regression is beneficial for stabilizing the model and reducing the variance of the estimates. This model has a test accuracy of XX.  (Describe the model interpretation if possible)
(Description)
```{r message=FALSE}
#### ridge 

library(tidymodels)
library(glmnet)

set.seed(123) # for reproducibility
train_data <- final_df_training_data
test_data <- test_data_df

recipe <- recipe(outcome ~ ., data = train_data) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric(), -all_outcomes())

# Model specification for multinomial logistic regression with Ridge
ridge_spec <- multinom_reg(penalty = tune(), mixture = 0) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# Workflow
workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(ridge_spec)

cv <- vfold_cv(train_data, v = 5, strata = outcome)
penalty_range <- dials::penalty(range = c(-6, -1), trans = log10_trans())

# Tuning
tune_results <- tune_grid(
  workflow,
  resamples = cv,
  grid = dials::grid_latin_hypercube(penalty_range, size = 20),
  metrics = metric_set(yardstick::accuracy)
)

# To get the best result based on accuracy
best_results <- select_best(tune_results, "accuracy")
best_lambda <- best_results$penalty

# Making the model again using the best lambda
final_ridge_spec <- multinom_reg(penalty = best_lambda, mixture = 0) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# Refit on the whole training dataset
final_workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(final_ridge_spec) %>%
  fit(data = train_data)

predictions <- predict(final_workflow, new_data = test_data, type = "prob")

max_prob <- apply(predictions, 1, which.max)
test_data$.pred_class <- levels(test_data$outcome)[max_prob]

# Model metrics
metrics <- sum(test_data$.pred_class == test_data$outcome) / nrow(test_data)
print(metrics)


# to see the coefficients

best_lambda_numeric <- as.numeric(best_lambda)

final_model <- final_workflow %>%
  pull_workflow_fit() %>%
  pluck("fit")

# extract coefficients as a list of matrices
coefficients_list <- coef(final_model, s = best_lambda_numeric)


combined_coefficients <- data.frame()


for (class_name in names(coefficients_list)) {
    # convert to df
    class_coefficients_df <- as.data.frame(as.matrix(coefficients_list[[class_name]]))
    
    # add column with class name 
    class_coefficients_df$class <- class_name
    

    class_coefficients_df$predictor <- rownames(as.matrix(coefficients_list[[class_name]]))
    

    combined_coefficients <- rbind(combined_coefficients, class_coefficients_df)
}

# Rename the columns appropriately
colnames(combined_coefficients)[1] <- "estimate"

# need to use pivot longer for df
combined_coefficients_df <- combined_coefficients %>%
  pivot_wider(names_from = class, values_from = estimate)


kable(combined_coefficients_df, caption = "Ridge")


library(yardstick)

# Assuming test_data$.pred_class contains the predicted classes
# and test_data$outcome contains the actual outcomes
test_data$.pred_class <- as.factor(test_data$.pred_class)
conf_matrix <- conf_mat(test_data, truth = outcome, estimate = .pred_class)

# Alternatively, use the kable function from knitr for a nicer format in markdown documents
knitr::kable(conf_matrix$table, caption = "Confusion Matrix for Ridge Model")
```

## Elastic Net Regression

The last candidate model is elastic net regression, this model is another regularization technique that combines the penalties of both ridge regression and lasso regression. Similar to ridge regression, elastic net adds a penalty term to the ordinary least squares (OLS) objective function to prevent overfitting and reduce the variance of the model. This penalty term is a combination of the L1 norm (lasso penalty) and the L2 norm (ridge penalty), controlled by two hyperparameters: alpha and lambda. This approach helps address multicollinearity issues and performs variable selection by encouraging sparsity while also maintaining the stability provided by Ridge regression. 

(Describe model and talk about test accuracy)
```{r message = FALSE}
####### Elastic Net

library(tidymodels)
library(glmnet)


# reproducibility/splitting the data

set.seed(123) # for reproducibility
train_data <- final_df_training_data
test_data <- test_data_df


recipe <- recipe(outcome ~ ., data = train_data) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric(), -all_outcomes())

# making the model specification for multinomial logistic regression using elastic net
elastic_net_spec <- multinom_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# workflow
workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(elastic_net_spec)

cv <- vfold_cv(train_data, v = 5, strata = outcome)

# setting up the tuning grid to use for the tuning
grid <- crossing(
  penalty = seq(-6, -1, length.out = 5) %>% map(~10^.) %>% unlist(),
  mixture = seq(0, 1, length.out = 5)
)

# tuning
tune_results <- tune_grid(
  workflow,
  resamples = cv,
  grid = grid,
  metrics = metric_set(yardstick::accuracy)
)

# to get the best result based on accuracy 
best_results <- select_best(tune_results, "accuracy")
best_lambda <- best_results$penalty
best_mixture <- best_results$mixture

# making the model again using the best lambda and mixture
final_elastic_net_spec <- multinom_reg(penalty = best_lambda, mixture = best_mixture) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# refit on the whole training dataset
final_workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(final_elastic_net_spec) %>%
  fit(data = train_data)

predictions <- predict(final_workflow, new_data = test_data, type = "prob")

# getting the hoghest probability prediction
max_prob <- apply(predictions %>% select(starts_with(".pred_")), 1, which.max)
test_data$.pred_class <- levels(test_data$outcome)[max_prob]

# getting the model metrics
metrics <- sum(test_data$.pred_class == test_data$outcome) / nrow(test_data)
print(metrics)


# To see the coefficients

# to see the coefficients

best_lambda_numeric <- as.numeric(best_lambda)

final_model <- final_workflow %>%
  pull_workflow_fit() %>%
  pluck("fit")

# extract coefficients as a list of matrices
coefficients_list <- coef(final_model, s = best_lambda_numeric)


combined_coefficients <- data.frame()


for (class_name in names(coefficients_list)) {
    # convert to df
    class_coefficients_df <- as.data.frame(as.matrix(coefficients_list[[class_name]]))
    
    # add column with class name 
    class_coefficients_df$class <- class_name
    

    class_coefficients_df$predictor <- rownames(as.matrix(coefficients_list[[class_name]]))
    

    combined_coefficients <- rbind(combined_coefficients, class_coefficients_df)
}

# Rename the columns appropriately
colnames(combined_coefficients)[1] <- "estimate"

# need to use pivot longer for df
combined_coefficients_df <- combined_coefficients %>%
  pivot_wider(names_from = class, values_from = estimate)


kable(combined_coefficients_df, caption = "Elastic Net")

library(yardstick)

# Assuming test_data$.pred_class contains the predicted classes
# and test_data$outcome contains the actual outcomes
test_data$.pred_class <- as.factor(test_data$.pred_class)
conf_matrix <- conf_mat(test_data, truth = outcome, estimate = .pred_class)

# Alternatively, use the kable function from knitr for a nicer format in markdown documents
knitr::kable(conf_matrix$table, caption = "Confusion Matrix for Elastic Net Model")
```

## Best Model: High-Dimensional


# Model: Predictive Accuracy
The aim for this model is to maximize the predictive accuracy on the test data, without focusing on interpretabiltiy. Here, we will focus on some more complex models compared to the baseline to see if they can achieve a greater prediction accuracy. 

## Random Forest
The first model that we are going to try to maximise the predictive accuracy is random forest. This is an ensemble method that builds upon the single decision tree model, Random Forest improves predictive accuracy by reducing overfitting through constructing a multitude of decision trees during training and outputs the mode of these classes. Each tree in the forest is built using a random subset of the training data and a random subset of the features. This randomness helps to decorrelate the individual trees, reducing the risk of overfitting. During prediction, each decision tree "votes" for the class it predicts based on the input variables. When making a prediction for a new data point, the Random Forest aggregates these individual votes to determine the final predicted class.

(Talk about why its suitable for the dataset)

Tuning using grid search and 10-fold cross validation was done to find the most optimal hyperparameters to maximise the test accuracy. Overall, this model achieved a test accuracy of XXX, which is .......
```{r}
# using the code from the lecture slides/previous problem set

set.seed(123) # Ensure reproducibility



data_train <- final_df_training_data
data_test <- test_data_df

# Define your recipe
recipe <- recipe(outcome ~ ., data = data_train) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())# %>%
  #prep()

# Prepare cross-validation for tuning
data_cv <- vfold_cv(data_train, v = 10, strata = outcome)

# Define the model spec with tuning placeholders
rf_spec <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("randomForest") %>%
  set_mode("classification")

# Create the workflow
workflow_rf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(rf_spec)

# Define the tuning grid
grid <- grid_regular(
  mtry(range = c(1, ncol(data_train) - 1)),
  trees(c(50, 1000)),
  min_n(range = c(2, 20)),
  levels = 1
)

# Perform the tuning
fit_rf <- tune_grid(
  workflow_rf,
  resamples = data_cv,
  grid = grid,
  metrics = metric_set(accuracy)
)

# Extract the best parameters
rf_best <- select_best(fit_rf, "accuracy")

# Finalize the model with the best parameters
rf_final <- finalize_workflow(workflow_rf, rf_best)

# Fit the finalized model on the entire training data
final_fit <- fit(rf_final, data = data_train)

# Evaluate the model on the test set
predictions <- predict(final_fit, new_data = data_test, type = "prob")
# Assuming the outcome variable in your test set is named "outcome"
test_data$predicted_class <- predict(final_fit, new_data = data_test, type = "class")$.pred_class

# Evaluate metrics
metrics <- yardstick::metrics(test_data, truth = outcome, estimate = predicted_class)
conf_mat <- yardstick::conf_mat(test_data, truth = outcome, estimate = predicted_class)

# Print the results
knitr::kable(metrics, caption = "Metrics for Random Forest Model")
knitr::kable(conf_mat$table, caption = "Confusion Matrix for Random Forest Model")
```

```{r}
rf_best %>%
  knitr::kable()
```

```{r}
fit_rf %>% autoplot()
```

## XGBoost
Our next model focussing on predictive accuraacy is XGBoost. It operates by sequentially constructing decision trees, where each subsequent tree aims to correct the errors made by the previous ones. This process is achieved through gradient descent optimization, which minimizes the disparity between predicted and actual outcomes. XGBoost also incorporates regularization techniques, such as penalties and constraints, which help prevent overfitting and improve model generalization. 

(Talk about why its suitable for the dataset, why xgboost?)

Similarly, **10 fold cross validation** and grid search were employed to tune the hyperparameters and select the most optimal ones. Using XGBoost resulted in a testing accuraacy of XXX. 
```{r}
# using the code from the lecture slides/previous problem set

set.seed(123) # for reproducibility
data_train <- final_df_training_data
data_test <- test_data_df
data_cv <- vfold_cv(data_train, v = 10)

boost <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  learn_rate = tune()
   
) %>%
  set_mode("classification") %>%
  set_engine("xgboost", objective = "multi:softprob")

## create this to use in naming in step_dummy
dummy_names_2 <- function (var, lvl, ordinal = FALSE, sep = "") 
{
    if (!ordinal) 
        nms <- paste(var, make.names(lvl), sep = sep)
    else nms <- paste0(var, names0(length(lvl), sep))
    nms
}

data_recipe <- recipe(outcome ~ ., data = data_train) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE, naming = dummy_names_2)# %>%
  #prep()

# workflow
workflow_boost <- workflow() %>%
  add_recipe(data_recipe) %>%
  add_model(boost)

grid_boost <- grid_regular(
  trees(range = c(1, 230)),
  tree_depth(range = c(1, 17)),
  learn_rate(range = c(-2, -0.001)),
  levels = 5
)

# tuning
fit_boost <- tune_grid(
  workflow_boost,
  resamples = data_cv,
  grid = grid_boost,
  metrics = metric_set(yardstick::accuracy)
)

# best model
boost_best <- fit_boost %>%
  select_best("accuracy")


boost_final <- boost %>%
  set_args(trees = boost_best$trees,
           tree_depth = boost_best$tree_depth,
           learn_rate = boost_best$learn_rate)

# Create a new workflow with the final model and the recipe
final_workflow <- workflow() %>%
  add_recipe(data_recipe) %>%
  add_model(boost_final)

# Fit the final workflow to the training data
final_fit <- final_workflow %>%
  fit(data = data_train)

# Predict on the test data
predictions <- predict(final_fit, new_data = data_test, type = "prob")

# Convert predictions to a class prediction if needed
data_test$.pred_class <- predict(final_fit, new_data = data_test, type = "class")$.pred_class

# Assuming 'outcome' is the name of the outcome variable in your test data
metrics <- yardstick::metrics(data_test, truth = outcome, estimate = .pred_class)
conf_mat <- yardstick::conf_mat(data_test, truth = outcome, estimate = .pred_class)

# Print the results
knitr::kable(metrics, caption = "Metrics for XGBoost Model")
knitr::kable(conf_mat$table, caption = "Confusion Matrix for XGBoost Model")

```

```{r}
boost_best  %>%
  knitr::kable()
```

```{r}
fit_boost %>% autoplot(resize.extra = 10)
```

## SVM
Another method we tried was using Support Vector Machines (SVM). This model finds the best line or boundary to separate different groups of data points with the maximum margin. These groups are determined by points closest to the decision boundary, called support vectors. SVMs can handle both linearly separable and non-linearly separable datasets through the use of kernel functions, which map the original input space into a higher-dimensional feature space where separation becomes possible. Similar to before, grid serach and 10-fold cross validation was used to find the most optimal hyperparameters. For this dataset, using SVM achieves a test prediction accuracy rate of XXX. (confusion matrix, etc...)

(Talk about why its suitable for the dataset)

```{r}
########### SVM

library(tidymodels)
#install.packages("kernlab")
library(kernlab) # For svm


# reproducibility/splitting the data

set.seed(123) # for reproducibility
train_data <- final_df_training_data
test_data <- test_data_df

recipe <- recipe(outcome ~ ., data = train_data) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric(), -all_outcomes())

# model specification for SVM
svm_spec <- svm_poly(cost = tune(), degree = tune()) %>%
  set_engine("kernlab") %>%
  set_mode("classification")

workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(svm_spec)

cv <- vfold_cv(train_data, v = 10, strata = outcome)

# setting up the tuning grid, only tuning the cost parameter 
grid <- expand_grid(cost = 10^seq(-3, 2, length.out = 5),
                    degree = c(1, 2, 3))

# tuning
tune_results <- tune_grid(
  workflow,
  resamples = cv,
  grid = grid,
  metrics = metric_set(yardstick::accuracy)
)

# getting best results based on accuracy
best_results <- select_best(tune_results, "accuracy")
best_cost <- best_results$cost
best_degree <- best_results$degree

# the best model with the best cost and degree
final_svm_spec <- svm_poly(cost = best_cost, degree = best_degree) %>%
  set_engine("kernlab") %>%
  set_mode("classification")

# refit on whole dataset 
final_workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(final_svm_spec) %>%
  fit(data = train_data)

predictions <- predict(final_workflow, new_data = test_data, type = "prob")


# getting the class with highest probability
test_data_predictions <- test_data %>%
  select(outcome) %>%
  bind_cols(predictions) %>%
  mutate(pred_class = max.col(select(., -outcome), ties.method = "first"))

# to convert back to factor 
levels_pred <- levels(test_data$outcome)
test_data_predictions$pred_class <- factor(test_data_predictions$pred_class, levels = 1:length(levels_pred), labels = levels_pred)


metrics <- yardstick::metrics(test_data_predictions, truth = outcome, estimate = pred_class)
conf_mat <- yardstick::conf_mat(test_data_predictions, truth = outcome, estimate = pred_class)

# Print the metrics and confusion matrix
knitr::kable(metrics, caption = "Metrics for SVM Model")
knitr::kable(conf_mat$table, caption = "Confusion Matrix for SVM Model")
```

## Naive Bayes
(write something)
```{r message=FALSE}
library(naivebayes)
library(caret)
library(randomForest)

### naive bayes

set.seed(123) # for reproducibility
train <- final_df_training_data 
test <- test_data_df


#Build model
bayes_model <- naive_bayes(outcome ~ ., data = train, laplace = TRUE) #, usekernel = T) 

#See accuracy on Training Data:
bayes_predictions_train <- predict(bayes_model, newdata = train[,-which(names(train)=="outcome")])

accuracy_train <- caret::confusionMatrix(bayes_predictions_train, train$outcome)

#See results on Testing Data:

bayes_predictions <- predict(bayes_model, newdata = test[,-which(names(test)=="outcome")])
accuracy_test <- caret::confusionMatrix(bayes_predictions, test$outcome)

#accuracy_train

accuracy_test  
```


## Best Model: Predictive Accuracy
(explain and show some plots)


### Without Merging 
complete case data 
(explain)
```{r}
# for unmerged-complete case

# Removing columns with more than 15% missing values

nas_per_column <- sapply(pre_merge_df, function(x) sum(is.na(x)) / nrow(pre_merge_df))
columns_to_remove <- names(nas_per_column[nas_per_column > 0.15])

pre_merge_df_clean <- pre_merge_df[, !(names(pre_merge_df) %in% columns_to_remove)]

unmerged_complete_case_df <- pre_merge_df_clean[complete.cases(pre_merge_df_clean), ]

```


(explain)
```{r}
####### ELastic Net

library(tidymodels)
library(glmnet)


# reproducibility/splitting the data

set.seed(123) # for reproducibility
data_split <- initial_split(unmerged_complete_case_df, prop = 0.75)
train_data <- training(data_split)
test_data <- testing(data_split)


recipe <- recipe(outcome ~ ., data = train_data) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric(), -all_outcomes())

# making the model specification for multinomial logistic regression using elastic net
elastic_net_spec <- multinom_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# workflow
workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(elastic_net_spec)

cv <- vfold_cv(train_data, v = 5, strata = outcome)

# setting up the tuning grid to use for the tuning
grid <- crossing(
  penalty = seq(-6, -1, length.out = 5) %>% map(~10^.) %>% unlist(),
  mixture = seq(0, 1, length.out = 5)
)

# tuning
tune_results <- tune_grid(
  workflow,
  resamples = cv,
  grid = grid,
  metrics = metric_set(yardstick::accuracy)
)

# to get the best result based on accuracy 
best_results <- select_best(tune_results, "accuracy")
best_lambda <- best_results$penalty
best_mixture <- best_results$mixture

# making the model again using the best lambda and mixture
final_elastic_net_spec <- multinom_reg(penalty = best_lambda, mixture = best_mixture) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# refit on the whole training dataset
final_workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(final_elastic_net_spec) %>%
  fit(data = train_data)

predictions <- predict(final_workflow, new_data = test_data, type = "prob")

# getting the hoghest probability prediction
max_prob <- apply(predictions %>% select(starts_with(".pred_")), 1, which.max)
test_data$.pred_class <- levels(test_data$outcome)[max_prob]

# getting the model metrics
metrics <- sum(test_data$.pred_class == test_data$outcome) / nrow(test_data)
print(metrics)

# to see the coefficients

best_lambda_numeric <- as.numeric(best_lambda)

final_model <- final_workflow %>%
  pull_workflow_fit() %>%
  pluck("fit")

# extract coefficients as a list of matrices
coefficients_list <- coef(final_model, s = best_lambda_numeric)


combined_coefficients <- data.frame()


for (class_name in names(coefficients_list)) {
    # convert to df
    class_coefficients_df <- as.data.frame(as.matrix(coefficients_list[[class_name]]))
    
    # add column with class name 
    class_coefficients_df$class <- class_name
    

    class_coefficients_df$predictor <- rownames(as.matrix(coefficients_list[[class_name]]))
    

    combined_coefficients <- rbind(combined_coefficients, class_coefficients_df)
}

# Rename the columns appropriately
colnames(combined_coefficients)[1] <- "estimate"

# need to use pivot longer for df
combined_coefficients_df <- combined_coefficients %>%
  pivot_wider(names_from = class, values_from = estimate)


kable(combined_coefficients_df, caption = "Elastic Net")

library(yardstick)

# Assuming test_data$.pred_class contains the predicted classes
# and test_data$outcome contains the actual outcomes
test_data$.pred_class <- as.factor(test_data$.pred_class)
conf_matrix <- conf_mat(test_data, truth = outcome, estimate = .pred_class)

# Alternatively, use the kable function from knitr for a nicer format in markdown documents
knitr::kable(conf_matrix$table, caption = "Confusion Matrix for Elastic Net Model")

```

(explain)
```{r}
# Creating a dataframe with model names and their accuracies
df1 <- data.frame(
  Model = c("multinominal", "single_tree", "random_forest", "xgb_boost",
            "lasso", "lasso_interaction_age_surgery", "ridge", "elastic_net", "svm", 
            "stochastic_gradient_descent", "bayes_naive_model"),
  Accuracy = c(0.78, 0.82, 0.80, 0.82, 0.82, 0.82, 0.76, 0.86, 0.78, 0.63, 0.68)
)

knitr::kable(df1, caption = "Model Accuracies using unmerged_complete_case (without SMOTE)")


```

# Findings
(explain)
```{r}
# Create a dataframe
df2 <- data.frame(
  Model = c("multinominal", "single_tree", "random_forest", "xgb_boost", "lasso",
            "lasso_interaction_age_surgery", "ridge", "elastic_net", "svm",
            "stochastic_gradient_descent", "bayes_naive_model"),
  complete_case = c(0.65, 0.63, 0.67, 0.63, 0.65, 0.65, 0.65, 0.65, 0.63, 0.59, 0.59),
  missForest = c(0.65, 0.71, 0.73, 0.73, 0.68, 0.69, 0.69, 0.71, 0.68, 0.51, 0.69),
  mean_mode_impute = c(0.65, 0.67, 0.72, 0.68, 0.69, 0.69, 0.67, 0.69, 0.63, 0.5, 0.68)
)

# View the dataframe

knitr::kable(df2, caption = "Model Accuracies Across Different Imputation Methods (with SMOTE)")
```


```{r}
df3 <- data.frame(
  Model = c(
    "multinominal", "single_tree", "random_forest", "xgb_boost",
    "lasso", "lasso interaction (age*surgery)", "ridge",
    "elastic_net", "svm", "stochastic_gradient_descent", "bayes_naive_model"
  ),
  complete_case = c(0.76, 0.69, 0.67, 0.65, 0.74, 0.76, 0.72, 0.72, 0.7, 0.63, 0.63),
  missforest = c(0.64, 0.68, 0.71, 0.75, 0.73, 0.73, 0.72, 0.72, 0.68, 0.6, 0.72),
  mean_mode_impute = c(0.69, 0.59, 0.69, 0.75, 0.73, 0.72, 0.68, 0.71, 0.67, 0.58, 0.65)
)

# Print the dataframe
knitr::kable(df3, caption = "Model Accuracies Across Different Imputation Methods (without SMOTE)")

```

# Conclusions
(summarise the ideal models for each section and some overall findings)

# Limitations
(Limitations and further steps: Missing can be coded as a category, best subset and parameter selection.)

